{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let $X$ be an $n_x$-dimensional vector representing the input data. There are $n_x$ features. Similarly, $Y$ is a $n_y$-dimenstional vector representing the outut. For binary classification $n_y=1$.\n",
    "\n",
    "There are $L$ layers with $n^{[l]}$ nodes in each. The input later ($X$) is considered layer zero, while the output layer is layer $L$.\n",
    "The linear part of layer $l$ is written\n",
    "$$ Z^{[l]}_{i} = W^{[l]}_{ij} A^{[l-1]}_{j} + b^{[L]}_i$$\n",
    "\n",
    "Here $Z^{[l]}_{i}$ refers to the linear part of the ith node in layer $l$. There is one $Z$ vector for each layer and it has dimension $n_l$. Here $W \\sim n_l \\times n_{l-1}$ and $b \\sim n_l$ are parameters.\n",
    "\n",
    "The activations of a layer $l$ are\n",
    "$$ A^{[l]}_i = g^{(l)}(Z^{[l]}_i)$$\n",
    "where the activation function $g$ (relu, sigmoid, tanh or whatever) is applied element-wise.\n",
    "\n",
    "We write the cost for a single training example as\n",
    "$$J = - \\large{(} \\small Y \\log\\left(A^{[L]}\\right) + (1-Y) \\log\\left(1- A^{[L]}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "Our goal is to compute the derivatives of $J$ wrt the parameters $W$ and $b$. We do this via repeated application of the chain rule. Our approach is as follows:\n",
    "\n",
    "1. calculate the derivative of $J$ wrt $A^{[L]}$ and $Z^{[L]}$\n",
    "2. use this to calculate the derivatives of $J$ wrt $W^{[L]}$ abd $b^{[L]}$\n",
    "3. show that given the derivatives for layer $l$ we can calculate all the derivatives for layer $l-1$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial A^{[L]}} = - \\Big( \\frac{Y_i}{A^{[L]}} - \\frac{(1-Y)}{(1-A^{[L]})} \\Big) = \\frac{ A^{[L]} - Y}{ A^{[L]} ( 1 - A^{[L]} )}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial Z^{[L]}} &= \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} = \\frac{\\partial J}{\\partial A^{[L]}} g'^{[L]}(Z^{[L]}) \\\\\n",
    "&= \\frac{\\partial J}{\\partial A^{[L]}} A^{[L]} ( 1 - A^{[L]} ) \\\\\n",
    "&= A^{[L]} - Y\n",
    "\\end{align}\n",
    "$$\n",
    "(last two lines only valid for sigmoid activation in layer $L$). Once we have this we can proceed to calculate the partial derivatives wrt $W$ and $b$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[l]}_{ij}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} \\frac{\\partial Z^{[l]}_i}{\\partial W^{[l]}_{ij}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} A^{[l-1]}_j\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{[l]}_{i}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} \\frac{\\partial Z^{[l]}_i}{\\partial b^{[l]}_{i}} = \\frac{\\partial J}{\\partial Z^{[l]}_i}\n",
    "$$\n",
    "Finally for $l<L$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial A^{[l]}_i} &= \\frac{\\partial J}{\\partial Z^{[l+1]}_j} \\frac{\\partial Z^{[l+1]}_j}{\\partial A^{[l]}_i}\\\\\n",
    "&= \\frac{\\partial J}{\\partial Z^{[l+1]}_j} W^{[l+1]}_{ji}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}_i} &= \\frac{\\partial J}{\\partial A^{[l]}_i} \\frac{\\partial A^{[l]}_i}{\\partial Z^{[l]}_i}\\\\\n",
    "&= \\frac{\\partial J}{\\partial A^{[l]}_i} g'^{[l]}(Z^{[l]}_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "m = 10  # number of training examples\n",
    "n_x = 2 # number of features\n",
    "n_y = 1 \n",
    "layer_dims = [ 5, 2, 1 ]\n",
    "activation_functions = ['relu', 'relu', 'sigmoid']\n",
    "#layer_dims = [ 5, 1 ]\n",
    "#activation_functions = ['relu', 'sigmoid']\n",
    "\n",
    "learning_rate = 0.1\n",
    "X = np.random.rand(n_x, m)\n",
    "Y = np.random.rand(n_y, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ret = np.maximum(0,x)\n",
    "    return(ret)\n",
    "\n",
    "def sigmoid(x):\n",
    "    ret = 1 + np.exp(-x)\n",
    "    return( 1/ret)\n",
    "\n",
    "def compute_cost(y_label, yhat):\n",
    "    size = y_label.shape[1]\n",
    "    ret = np.dot(y_label, np.log(yhat).T) + np.dot((1-y_label), np.log(1-yhat).T)\n",
    "    ret = ret.squeeze()\n",
    "    return( -ret/size )\n",
    "\n",
    "def initialize_params(n_x, layer_dims):\n",
    "    np.random.seed(0)\n",
    "    layer_dims = [n_x] + layer_dims\n",
    "    ret = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        n_h = layer_dims[i]\n",
    "        W = np.random.rand(n_h, layer_dims[i-1])\n",
    "        b = np.zeros( (n_h, 1) ) + 0.3\n",
    "        ret['W'+str(i)] = W\n",
    "        ret['b'+str(i)] = b\n",
    "    return(ret)\n",
    "\n",
    "def forward_prop(X, params):\n",
    "    assert( len(params)%2 == 0 )\n",
    "    L = int(len(params)/2)\n",
    "    ret = {}\n",
    "    cache = {}\n",
    "    A_prev = X\n",
    "    for i in range(1,L+1):\n",
    "        W = params['W'+str(i)]\n",
    "        b = params['b'+str(i)]\n",
    "        Z = np.dot( W, A_prev) + b\n",
    "        activation_function = activation_functions[i-1]\n",
    "        if activation_function == 'relu':\n",
    "            A = relu(Z)\n",
    "        elif activation_function == 'sigmoid':\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            raise ValueError('unknown activation function')\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "        A_prev = A\n",
    "    return(A, cache)\n",
    "\n",
    "def relu_deriv(v):\n",
    "    return( np.greater(v,0))\n",
    "\n",
    "def compute_grads(AL, params, cache, X, Y):\n",
    "    # X is n_x x m\n",
    "    # Y is 1 x m\n",
    "    cache['A0'] = X\n",
    "    assert( len(params)%2 == 0 )\n",
    "    L = int(len(params)/2)\n",
    "    \n",
    "    grads = {}\n",
    "    dZ = AL - Y\n",
    "    for i in reversed(range(1,L+1)):\n",
    "        A_next = cache['A'+str(i-1)]\n",
    "        dW = np.dot(dZ, A_next.T)/m  \n",
    "        db = np.sum(dZ, axis=1, keepdims = True)/m \n",
    "        grads['dW'+str(i)] = dW\n",
    "        grads['db'+str(i)] = db\n",
    "        if i>1:\n",
    "            W = params['W'+str(i)]\n",
    "            Z_next = cache['Z'+str(i-1)]\n",
    "            dZ = np.dot(W.T, dZ) * relu_deriv(Z_next)\n",
    "        \n",
    "    return( grads )\n",
    "\n",
    "def update_parameters( parameters, gradients, learning_rate):\n",
    "    new_params = {}\n",
    "    for param_name, param_value in parameters.items():\n",
    "        grad = grads['d'+param_name]\n",
    "        new_params[param_name] = param_value - learning_rate * grad\n",
    "        #print( \"updating {} from\\n {} \\nto\\n {}\".format(param_name, param_value, new_params[param_name] ) )\n",
    "    return( new_params )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "params = initialize_params(n_x, layer_dims)\n",
    "num_iterations = 5000\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward prop\n",
    "    AL, cache = forward_prop(X, params)\n",
    "\n",
    "    # compute cost\n",
    "    cost = compute_cost(Y, AL)\n",
    "    #print(\"Cost after iteration {}: {}\".format( i, cost) )\n",
    "    # backward prop to calculate gradients\n",
    "    grads = compute_grads(AL, params, cache, X, Y)\n",
    "\n",
    "    params = update_parameters( params, grads, learning_rate )\n",
    "print( \"Final cost: {}\".format(cost))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
