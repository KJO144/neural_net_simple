{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let $X$ be an $n_x$-dimensional vector representing the input data. There are $n_x$ features. Similarly, $Y$ is a $n_y$-dimenstional vector representing the outut. For binary classification $n_y=1$.\n",
    "\n",
    "There are $L$ layers with $n^{[l]}$ nodes in each. The input later ($X$) is considered layer zero, while the output layer is layer $L$.\n",
    "The linear part of layer $l$ is written\n",
    "$$ Z^{[l]}_{i} = W^{[l]}_{ij} A^{[l-1]}_{j} + b^{[L]}_i$$\n",
    "\n",
    "Here $Z^{[l]}_{i}$ refers to the linear part of the ith node in layer $l$. There is one $Z$ vector for each layer and it has dimension $n_l$. Here $W \\sim n_l \\times n_{l-1}$ and $b \\sim n_l$ are parameters.\n",
    "\n",
    "The activations of a layer $l$ are\n",
    "$$ A^{[l]}_i = g^{(l)}(Z^{[l]}_i)$$\n",
    "where the activation function $g$ (relu, sigmoid, tanh or whatever) is applied element-wise.\n",
    "\n",
    "We write the cost for a single training example as\n",
    "$$J = - \\large{(} \\small Y \\log\\left(A^{[L]}\\right) + (1-Y) \\log\\left(1- A^{[L]}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "Our goal is to compute the derivatives of $J$ wrt the parameters $W$ and $b$. We do this via repeated application of the chain rule. Our approach is as follows:\n",
    "\n",
    "1. calculate the derivative of $J$ wrt $A^{[L]}$ and $Z^{[L]}$\n",
    "2. use this to calculate the derivatives of $J$ wrt $W^{[L]}$ abd $b^{[L]}$\n",
    "3. show that given the derivatives for layer $l$ we can calculate all the derivatives for layer $l-1$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial A^{[L]}} = - \\Big( \\frac{Y_i}{A^{[L]}} - \\frac{(1-Y)}{(1-A^{[L]})} \\Big) = \\frac{ A^{[L]} - Y}{ A^{[L]} ( 1 - A^{[L]} )}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial Z^{[L]}} &= \\frac{\\partial J}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} = \\frac{\\partial J}{\\partial A^{[L]}} g'^{[L]}(Z^{[L]}) \\\\\n",
    "&= \\frac{\\partial J}{\\partial A^{[L]}} A^{[L]} ( 1 - A^{[L]} ) \\\\\n",
    "&= A^{[L]} - Y\n",
    "\\end{align}\n",
    "$$\n",
    "(last two lines only valid for sigmoid activation in layer $L$). Once we have this we can proceed to calculate the partial derivatives wrt $W$ and $b$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[l]}_{ij}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} \\frac{\\partial Z^{[l]}_i}{\\partial W^{[l]}_{ij}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} A^{[l-1]}_j\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{[l]}_{i}} = \\frac{\\partial J}{\\partial Z^{[l]}_i} \\frac{\\partial Z^{[l]}_i}{\\partial b^{[l]}_{i}} = \\frac{\\partial J}{\\partial Z^{[l]}_i}\n",
    "$$\n",
    "Finally for $l<L$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial A^{[l]}_i} &= \\frac{\\partial J}{\\partial Z^{[l+1]}_j} \\frac{\\partial Z^{[l+1]}_j}{\\partial A^{[l]}_i}\\\\\n",
    "&= \\frac{\\partial J}{\\partial Z^{[l+1]}_j} W^{[l+1]}_{ji}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}_i} &= \\frac{\\partial J}{\\partial A^{[l]}_i} \\frac{\\partial A^{[l]}_i}{\\partial Z^{[l]}_i}\\\\\n",
    "&= \\frac{\\partial J}{\\partial A^{[l]}_i} g'^{[l]}(Z^{[l]}_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
